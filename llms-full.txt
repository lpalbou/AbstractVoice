# AbstractVoice: Developer Integration Guide

## OVERVIEW

AbstractVoice is a production-ready Python library for offline voice interactions. This guide provides comprehensive technical documentation for developers and architects integrating voice capabilities into applications.

## CORE FEATURES

### 🎯 **Offline-First Architecture**
- **Complete Offline Operation**: Works 100% offline after initial model download (~6GB cache)
- **No API Dependencies**: No internet, API keys, or usage limits required
- **Privacy-First**: All processing happens locally, zero data transmission

### 🔊 **High-Quality Text-to-Speech**
- **Premium Models**: VITS with natural prosody (requires espeak-ng)
- **Universal Fallback**: Tacotron2 models work everywhere without dependencies
- **Smart Detection**: Automatic model selection based on system capabilities
- **Speed Control**: Pitch-preserving adjustment (0.5x-2.0x) with librosa
- **Professional Audio**: <20ms pause/resume, seamless streaming

### 🎤 **Speech Recognition**
- **Engine**: OpenAI Whisper (offline models: tiny→large)
- **Voice Activity Detection**: WebRTC VAD for efficient processing
- **Interrupt Handling**: Automatic TTS pause when user speaks
- **Multiple Modes**: Conversation, wait, stop, push-to-talk

### 🌍 **Multilingual Support**
- **Languages**: English, French, Spanish, German, Italian (5 languages total)
- **Voice Selection**: Multiple voice options per language with gender/accent variants
- **Dynamic Switching**: Runtime language changes without restart
- **Note**: Russian is not currently supported despite appearing in CLI help

## INSTALLATION

### **Quick Start**
```bash
# Recommended: All features included
pip install abstractvoice[all]

# Minimal core (2 dependencies only)
pip install abstractvoice

# Add features as needed
pip install abstractvoice[tts]      # Text-to-speech only
pip install abstractvoice[stt]      # Speech-to-text only
pip install abstractvoice[voice]    # Audio I/O
```

### **Language-Specific Installation**
```bash
# Single languages (all include the same dependencies)
pip install abstractvoice[fr]       # French
pip install abstractvoice[es]       # Spanish
pip install abstractvoice[de]       # German
pip install abstractvoice[it]       # Italian

# Multiple languages
pip install abstractvoice[fr,de,it]

# Note: All language extras install identical dependencies
# Language-specific models are downloaded at runtime
```

### **Quality Enhancement (Optional)**
```bash
# For premium voice quality (VITS models)
# macOS
brew install espeak-ng

# Linux
sudo apt-get install espeak-ng

# Windows
# Download and install espeak-ng-X64.msi from GitHub releases
```

### **Development Installation**
```bash
git clone https://github.com/lpalbou/abstractvoice.git
cd abstractvoice
pip install -e ".[all]"
```

## QUICK START EXAMPLES

### **1. Basic Setup**
```python
from abstractvoice import VoiceManager

# Simplest initialization (automatic model selection)
vm = VoiceManager()

# Language-specific setup
vm = VoiceManager(language='fr')    # French with optimal defaults
vm = VoiceManager(language='it')    # Italian with speed optimization
vm = VoiceManager(language='de')    # German with premium quality

# With debug information
vm = VoiceManager(debug_mode=True)
```

### **2. Smart Model Selection**
```python
# AbstractVoice automatically selects the best model for your system:
# - Tries VITS (premium quality) if espeak-ng available
# - Falls back to Tacotron2 (universal compatibility) if needed
# - Shows model selection in debug mode

vm = VoiceManager(language='en', debug_mode=True)
# Output: ✨ Using premium quality model: tts_models/en/ljspeech/vits
#         🌍 Using English voice: tts_models/en/ljspeech/vits
```

### **3. Voice Discovery and Selection**
```python
# List all available voices
vm = VoiceManager()
vm.list_voices()

# Browse specific language
vm.list_voices('fr')

# Set specific voice
vm.set_voice('fr', 'css10_vits')        # Premium French male voice
vm.set_voice('it', 'mai_male_vits')     # Premium Italian male (0.8x speed)
vm.set_voice('de', 'thorsten_vits')     # Premium German male voice
```

## CORE OPERATIONS

### **1. Text-to-Speech**
```python
# Basic speech synthesis
vm = VoiceManager()
success = vm.speak("Hello, I am your AI assistant!")

# With speed control (preserves pitch)
vm.speak("This is normal speed", speed=1.0)
vm.speak("This is faster", speed=1.5)
vm.speak("This is slower", speed=0.7)

# With completion callback
def on_complete():
    print("Speech finished!")

vm.speak("Text with callback", callback=on_complete)

# Check if currently speaking
if vm.is_speaking():
    print("TTS is active")
```

### **2. Language Support**
```python
# Language-specific instances
vm_en = VoiceManager(language='en')    # English
vm_fr = VoiceManager(language='fr')    # French
vm_es = VoiceManager(language='es')    # Spanish
vm_de = VoiceManager(language='de')    # German
vm_it = VoiceManager(language='it')    # Italian

# Speak in different languages
vm_fr.speak("Bonjour! Je suis votre assistant IA.")
vm_es.speak("¡Hola! Soy tu asistente de IA.")
vm_de.speak("Hallo! Ich bin Ihr KI-Assistent.")
vm_it.speak("Ciao! Sono il tuo assistente IA.")

# Dynamic language switching
vm = VoiceManager()
vm.set_language('fr')
vm.speak("Maintenant je parle français.")
vm.set_language('en')
vm.speak("Now I speak English.")
```

### **3. Professional Audio Control**
```python
# Immediate pause/resume (20ms response time)
vm.speak("This is a long text that can be paused and resumed seamlessly.")

# Pause (stops within 20ms)
if vm.pause_speaking():
    print("✅ Paused successfully")

# Resume from exact position (no repetition)
if vm.resume_speaking():
    print("✅ Resumed from exact position")

# Check states
if vm.is_speaking():
    print("Currently playing")
if vm.is_paused():
    print("Currently paused")

# Stop completely
vm.stop_speaking()
```

### **4. Voice Recognition (Speech-to-Text)**
```python
# Basic speech recognition
def handle_speech(text):
    print(f"User said: {text}")
    vm.speak(f"I heard: {text}")

def handle_stop():
    print("User said 'stop'")
    vm.stop_speaking()

# Start listening
vm.listen(
    on_transcription=handle_speech,
    on_stop=handle_stop
)

# Check listening state
if vm.is_listening():
    print("Actively listening")

# Stop listening
vm.stop_listening()
```

## ADVANCED FEATURES

### **CLI Interface**
```bash
# Voice mode (interactive conversation with AI)
abstractvoice

# CLI REPL example
abstractvoice-cli cli

# Web API server example
abstractvoice-cli web

# Simple example
abstractvoice-cli simple

# With specific language
abstractvoice --language fr
abstractvoice-cli cli --language de

# Check available options
abstractvoice --help
abstractvoice-cli --help
```

### **Voice Management**
```python
# Get current language information
info = vm.get_language_name()
print(f"Current: {info}")

# List supported languages
supported = vm.get_supported_languages()
print(f"Supported: {supported}")

# Change settings at runtime
vm.set_speed(1.2)           # 20% faster
vm.change_vad_aggressiveness(2)  # More sensitive voice detection
```

### **Voice Modes for Different Use Cases**
```python
# Different interaction modes
vm.set_voice_mode("full")   # Continuous listening, can interrupt TTS
vm.set_voice_mode("wait")   # Listen only when TTS is not playing
vm.set_voice_mode("stop")   # Stop TTS when user speaks
vm.set_voice_mode("ptt")    # Push-to-talk mode
```

## INTEGRATION PATTERNS

### **AI Assistant Integration**
```python
from abstractvoice import VoiceManager

class VoiceAI:
    def __init__(self, language='en'):
        self.vm = VoiceManager(language=language, debug_mode=False)
        self.conversation_active = False

    def start_conversation(self):
        """Start voice conversation with AI"""
        self.conversation_active = True
        self.vm.speak("Hello! I'm your voice assistant. How can I help you?")

        # Note: listen() blocks until stop command
        # In production, run this in a separate thread
        self.vm.listen(
            on_transcription=self.process_user_input,
            on_stop=self.end_conversation
        )

    def process_user_input(self, text):
        """Process user speech and respond"""
        if not self.conversation_active:
            return

        print(f"User said: {text}")

        # Stop current speech if user interrupts
        self.vm.stop_speaking()

        # Process with your AI system (replace with actual AI)
        ai_response = self.generate_response(text)

        # Speak response
        if self.conversation_active:
            self.vm.speak(ai_response)

    def generate_response(self, text):
        """Replace with your actual AI system (OpenAI, Ollama, etc.)"""
        # Simple example responses
        if "hello" in text.lower():
            return "Hello! Nice to meet you."
        elif "language" in text.lower():
            return "I can speak multiple languages. Try saying 'switch to French'."
        elif "french" in text.lower():
            self.vm.set_language('fr')
            return "Bonjour! Je parle français maintenant."
        else:
            return f"I heard you say: {text}. How can I help you with that?"

    def end_conversation(self):
        """End conversation gracefully"""
        self.conversation_active = False
        self.vm.speak("Goodbye! Have a great day!")

    def cleanup(self):
        """Clean up resources"""
        self.conversation_active = False
        self.vm.cleanup()

# Usage example
if __name__ == "__main__":
    ai = VoiceAI(language='en')
    try:
        ai.start_conversation()
    except KeyboardInterrupt:
        print("\nConversation interrupted")
    finally:
        ai.cleanup()
```

### **Streaming AI Responses**
```python
from abstractvoice import VoiceManager
import time

def stream_ai_with_voice(user_input, vm):
    """Stream AI response with immediate TTS for each complete sentence"""

    # Example streaming AI generator (replace with OpenAI, Ollama, etc.)
    def mock_ai_stream(text):
        """Simulate streaming AI response - replace with actual AI"""
        responses = [
            f"Let me think about your question: '{text}'. ",
            "Based on my understanding, ",
            "I can provide several insights. ",
            "First, this is an interesting topic. ",
            "Second, there are multiple perspectives to consider. ",
            "I hope this helps answer your question!"
        ]

        for response in responses:
            # Simulate streaming delay
            time.sleep(0.5)
            yield response

    # Stream and speak complete sentences immediately
    full_response = ""
    current_sentence = ""

    print(f"Processing: {user_input}")

    for chunk in mock_ai_stream(user_input):
        full_response += chunk
        current_sentence += chunk

        # Check for sentence boundaries
        if any(punct in chunk for punct in ['.', '!', '?']):
            # Find the sentence boundary
            for punct in ['.', '!', '?']:
                if punct in current_sentence:
                    # Get complete sentence up to punctuation
                    idx = current_sentence.rfind(punct)
                    if idx != -1:
                        sentence = current_sentence[:idx + 1].strip()
                        if sentence:
                            vm.speak(sentence)
                            print(f"Speaking: {sentence}")
                        current_sentence = current_sentence[idx + 1:].strip()
                        break

    # Speak any remaining text
    if current_sentence.strip():
        vm.speak(current_sentence.strip())

    return full_response

# Usage example
if __name__ == "__main__":
    vm = VoiceManager(language='en')

    user_question = "What can you tell me about artificial intelligence?"
    response = stream_ai_with_voice(user_question, vm)

    print(f"\nComplete response: {response}")
    vm.cleanup()
```

### **Multilingual AI Assistant**
```python
class MultilingualAssistant:
    def __init__(self):
        self.voices = {
            'en': VoiceManager(language='en'),
            'fr': VoiceManager(language='fr'),
            'es': VoiceManager(language='es'),
            'de': VoiceManager(language='de'),
            'it': VoiceManager(language='it')
        }
        self.current_language = 'en'

    def set_language(self, language):
        if language in self.voices:
            self.current_language = language
            return True
        return False

    def speak(self, text, language=None):
        lang = language or self.current_language
        if lang in self.voices:
            self.voices[lang].speak(text)

    def greet_in_all_languages(self):
        greetings = {
            'en': "Hello! I'm your AI assistant.",
            'fr': "Bonjour! Je suis votre assistant IA.",
            'es': "¡Hola! Soy tu asistente de IA.",
            'de': "Hallo! Ich bin Ihr KI-Assistent.",
            'it': "Ciao! Sono il tuo assistente IA."
        }

        for lang, greeting in greetings.items():
            self.speak(greeting, language=lang)
            time.sleep(3)

# Usage
assistant = MultilingualAssistant()
assistant.set_language('fr')
assistant.speak("Bienvenue!")
```

## PRODUCTION DEPLOYMENT

### **Error Handling & Fallbacks**
```python
def create_robust_voice_manager():
    """Production-ready VoiceManager with fallbacks"""
    try:
        # Try premium quality first
        vm = VoiceManager(debug_mode=False)
        print("✅ Voice system ready")
        return vm
    except Exception as e:
        print(f"⚠️ Voice initialization failed: {e}")
        return None

def safe_voice_operation(vm, operation, *args, **kwargs):
    """Safely execute voice operations"""
    try:
        if operation == "speak":
            return vm.speak(*args, **kwargs)
        elif operation == "pause":
            return vm.pause_speaking()
        elif operation == "resume":
            return vm.resume_speaking()
        elif operation == "stop":
            return vm.stop_speaking()
        return False
    except Exception as e:
        print(f"Voice operation failed: {e}")
        return False

# Usage
vm = create_robust_voice_manager()
if vm:
    success = safe_voice_operation(vm, "speak", "Hello world")
    if not success:
        print("Speech failed, using text fallback")
```

### **Resource Cleanup**
```python
def cleanup_voice_resources(vm):
    """Proper cleanup for production environments"""
    try:
        vm.stop_speaking()
        vm.stop_listening()
        vm.cleanup()
        print("✅ Voice resources cleaned up")
    except Exception as e:
        print(f"⚠️ Cleanup warning: {e}")

# Context manager for automatic cleanup
from contextlib import contextmanager

@contextmanager
def voice_manager_context(**kwargs):
    """Context manager for automatic resource management"""
    vm = VoiceManager(**kwargs)
    try:
        yield vm
    finally:
        cleanup_voice_resources(vm)

# Usage
with voice_manager_context(language='fr') as vm:
    vm.speak("Bonjour! This will auto-cleanup when done.")
```

## ARCHITECTURE NOTES

### **Model Storage & Caching**
- **Cache Location**: `~/Library/Application Support/tts/` (macOS), equivalent on other platforms
- **Storage Requirements**: ~6GB for all languages (one-time download)
- **Offline Operation**: 100% offline after initial model download
- **Model Selection**: Automatic VITS → Tacotron2 fallback based on espeak-ng availability

### **Dependencies**
```python
# Core dependencies (always required)
numpy>=1.24.0
requests>=2.31.0

# Optional dependencies (install as needed)
# Voice functionality
sounddevice>=0.4.6    # Audio I/O
webrtcvad>=2.0.10     # Voice activity detection
PyAudio>=0.2.13       # Audio interface
soundfile>=0.12.1     # Audio file handling

# TTS functionality
coqui-tts>=0.27.0     # Text-to-speech engine
torch>=2.0.0          # PyTorch for TTS models
torchaudio>=2.0.0     # Audio processing for PyTorch
librosa>=0.10.0       # Audio analysis

# STT functionality
openai-whisper>=20230314  # Speech recognition
tiktoken>=0.6.0       # Tokenizer for Whisper

# Web functionality
flask>=2.0.0          # Web API server
```

### **Performance Characteristics**
- **Model Loading**: 2-5 seconds (cached after first use)
- **Speech Synthesis**: Real-time to 2x real-time depending on model
- **Pause/Resume Latency**: <20ms
- **Memory Usage**: ~500MB-2GB depending on models loaded
- **CPU Usage**: Moderate (can run on CPU, GPU optional for speed)

## TROUBLESHOOTING

### **Common Issues**
1. **Import Errors**: Install missing optional dependencies
2. **Audio Issues**: Check `sounddevice.query_devices()`
3. **espeak-ng Missing**: VITS models fallback to Tacotron2 automatically
4. **Permission Errors**: Grant microphone access on macOS/Windows

### **Debug Mode**
```python
# Enable debug output to diagnose issues
vm = VoiceManager(debug_mode=True)
# Shows:
# ✨ Using premium quality model: tts_models/en/ljspeech/vits
# 🌍 Using English voice: tts_models/en/ljspeech/vits
# > Loading TTS model: tts_models/en/ljspeech/vits

# Check current state
print(f"Current language: {vm.get_language_name()}")
print(f"Supported languages: {vm.get_supported_languages()}")
print(f"Current speed: {vm.get_speed()}")

# Test basic functionality
result = vm.speak("Testing debug mode")
print(f"Speech result: {result}")
```

## LICENSING & COMPLIANCE

### **Voice Model Licenses**
- **VITS Models**: Require espeak-ng (GPL v3+ with linking exception)
- **Tacotron2 Models**: No additional dependencies, standard open source
- **Commercial Use**: ✅ All models support commercial applications
- **Attribution**: Required for some datasets (see docs/voices-and-licenses.md)

### **Deployment Guidelines**
- **Embedded Systems**: Use Tacotron2 models for universal compatibility
- **Enterprise**: VITS models provide best quality when espeak-ng can be installed
- **Cloud/Serverless**: Consider model download time on cold starts

---

**Complete documentation and examples available at:**
- **GitHub Repository**: https://github.com/lpalbou/abstractvoice
- **Voice Models & Licensing**: docs/voices-and-licenses.md
- **Installation Guide**: docs/installation.md
- **Architecture Documentation**: docs/architecture.md
- **CLI Reference**: Run `abstractvoice --help` and `abstractvoice-cli --help`
- **Voice Selection**: Use `/setvoice` command in CLI or `vm.list_voices()` in Python

**Quick verification of your installation:**
```bash
python -c "from abstractvoice import VoiceManager; vm = VoiceManager(); print('✅ AbstractVoice installed correctly')"
```
