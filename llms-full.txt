# AbstractVoice: Developer Integration Guide

## OVERVIEW

AbstractVoice is a production-ready Python library for offline voice interactions. This guide provides comprehensive technical documentation for developers and architects integrating voice capabilities into applications.

**Version 0.5.0** introduces automatic model downloads on language/voice switching, crash-safe memory management, and genuine voice diversity. No more silent failures - the system now clearly reports what's happening and downloads models automatically when needed.

## CORE FEATURES

### 🎯 **Offline-First Architecture**
- **Complete Offline Operation**: Works 100% offline after initial model download (~6GB cache)
- **No API Dependencies**: No internet, API keys, or usage limits required
- **Privacy-First**: All processing happens locally, zero data transmission

### 🔊 **High-Quality Text-to-Speech**
- **Premium Models**: VITS with natural prosody (requires espeak-ng)
- **Universal Fallback**: Tacotron2 models work everywhere without dependencies
- **Smart Detection**: Automatic model selection based on system capabilities
- **Speed Control**: Pitch-preserving adjustment (0.5x-2.0x) with librosa
- **Professional Audio**: <20ms pause/resume, seamless streaming

### 🎤 **Speech Recognition**
- **Engine**: OpenAI Whisper (offline models: tiny→large)
- **Voice Activity Detection**: WebRTC VAD for efficient processing
- **Interrupt Handling**: Automatic TTS pause when user speaks
- **Multiple Modes**: Conversation, wait, stop, push-to-talk

### 🌍 **Multilingual Support**
- **Languages**: English, French, Spanish, German, Italian (5 languages total)
- **Voice Selection**: Multiple voice options per language with gender/accent variants
- **Dynamic Switching**: Runtime language changes without restart
- **Note**: Russian is not currently supported despite appearing in CLI help

## INSTALLATION

### **Quick Start (v0.5.0+ with Auto-Download)**
```bash
# Complete voice functionality (recommended)
pip install abstractvoice[voice-full]

# Check installation and dependencies
abstractvoice check-deps

# Test functionality
abstractvoice simple

# Minimal core with essential TTS dependencies included
pip install abstractvoice
```

**New in v0.5.0:** Essential TTS dependencies now included in base installation. Models download automatically when switching languages/voices - no manual setup required!

### **Smart Dependency Groups**
```bash
# Complete voice features (TTS + STT + Audio)
pip install abstractvoice[voice-full]

# Lightweight options
pip install abstractvoice[core-tts]    # TTS only (no audio/STT)
pip install abstractvoice[core-stt]    # STT only (no TTS/audio)
pip install abstractvoice[audio-only]  # Audio processing only

# Legacy (still available)
pip install abstractvoice[all]         # Everything
pip install abstractvoice[tts]         # TTS functionality
pip install abstractvoice[stt]         # STT functionality
```

### **Dependency Conflict Resolution**
```bash
# If you get PyTorch/TorchVision errors:
pip uninstall torch torchvision torchaudio transformers
pip install abstractvoice[all]  # Installs tested compatible versions

# Check for conflicts
abstractvoice check-deps
```

### **Quality Enhancement (Optional)**
```bash
# For premium voice quality (VITS models)
# macOS
brew install espeak-ng

# Linux
sudo apt-get install espeak-ng

# Windows
# Download and install espeak-ng-X64.msi from GitHub releases
```

### **Development Installation**
```bash
git clone https://github.com/lpalbou/abstractvoice.git
cd abstractvoice
pip install -e ".[all,dev]"
```

### **Dependency Management (2024-2025 Updates)**

**Enhanced Error Handling**: AbstractVoice now detects specific PyTorch conflicts and provides precise fix instructions.

**Version Compatibility**: All PyTorch ecosystem packages are pinned to tested compatible versions:
- PyTorch: 2.0.0 - 2.3.x
- TorchVision: 0.15.0 - 0.18.x (now explicitly included)
- TorchAudio: 2.0.0 - 2.3.x
- Coqui-TTS: 0.27.0 - 0.29.x

**Dependency Checker**: New utility to diagnose installation issues:
```bash
# Command line (recommended)
abstractvoice check-deps

# Python API
from abstractvoice.dependency_check import check_dependencies
check_dependencies()  # Comprehensive compatibility report
```

## MODEL MANAGEMENT (v0.4.0+)

### **⚡ Instant TTS with Automatic Model Management**

**Version 0.4.0 Game Changer**: TTS now works instantly after first setup with automatic essential model download.

```python
from abstractvoice import VoiceManager

# Initialize - automatically downloads essential model (107MB) if needed
vm = VoiceManager()

# TTS works immediately!
vm.speak("Hello! TTS works out of the box!")
```

**What Changed in v0.4.0**:
- ✅ **Essential model auto-download**: Fast Pitch (107MB) downloads on first use
- ✅ **Permanent caching**: Models cached offline forever after first download
- ✅ **Instant subsequent usage**: ~200ms load time after caching
- ✅ **No complex setup**: No manual model management needed

### **Programmatic Model Management APIs**

**For Third-Party Applications** - Complete JSON APIs:

```python
from abstractvoice import list_models, download_model, get_status, is_ready
import json

# Quick readiness check
ready = is_ready()
print(f"TTS ready: {ready}")

# Get all models as JSON
models_json = list_models()
models = json.loads(models_json)
print(f"Available languages: {list(models.keys())}")

# Get specific language models
french_models = json.loads(list_models('fr'))
print(f"French voices: {list(french_models['fr'].keys())}")

# Download specific model (voice ID format)
success = download_model('fr.css10_vits')
print(f"Downloaded French VITS: {success}")

# Download with full model name
success = download_model('tts_models/de/thorsten/vits')
print(f"Downloaded German model: {success}")

# Get cache status
status = json.loads(get_status())
print(f"Total cached: {status['total_cached']} models")
print(f"Cache size: {status['total_size_mb']} MB")
print(f"Ready for offline: {status['ready_for_offline']}")
```

**For Library Integration** - VoiceManager APIs:

```python
from abstractvoice import VoiceManager

vm = VoiceManager()

# Check if ready for immediate use
ready = vm.is_model_ready()
print(f"Essential model ready: {ready}")

# Ensure TTS is ready (downloads if needed)
ready = vm.ensure_ready(auto_download=True)
print(f"System ready after ensure: {ready}")

# List available models with metadata
models = vm.list_available_models()
for lang, voices in models.items():
    print(f"{lang}: {len(voices)} voices")

# Download specific model
success = vm.download_model('de.thorsten_vits')
print(f"Downloaded German voice: {success}")

# Get cache status
status = vm.get_cache_status()
print(f"Cache info: {status['total_cached']} models, {status['total_size_mb']}MB")
```

### **CLI Model Management**

**Essential Model Download** (recommended first step):
```bash
# Download essential English model for offline use
abstractvoice download-models

# Check current status
abstractvoice download-models --status
```

**Language-Specific Downloads**:
```bash
# Download models for specific languages
abstractvoice download-models --language fr    # French
abstractvoice download-models --language de    # German
abstractvoice download-models --language it    # Italian
abstractvoice download-models --language es    # Spanish

# Download specific model by name
abstractvoice download-models --model tts_models/fr/css10/vits
```

**Advanced Options**:
```bash
# Download all available models (large!)
abstractvoice download-models --all

# Clear model cache
abstractvoice download-models --clear

# Debug mode
abstractvoice download-models --debug
```

### **Voice Selection in CLI**

```bash
# Start CLI REPL
abstractvoice cli

# List all available voices with download status
/setvoice

# Download and set specific voice (uses programmatic APIs internally)
/setvoice fr.css10_vits      # French CSS10 VITS
/setvoice de.thorsten_vits   # German Thorsten
/setvoice it.mai_male_vits   # Italian Male VITS
```

### **Model Information Reference**

**Essential Model**:
- `en.fast_pitch` (107MB) - Reliable English voice, no dependencies

**Premium Models**:
- `fr.css10_vits` (548MB) - High-quality French, requires espeak-ng
- `de.thorsten_vits` (548MB) - High-quality German, requires espeak-ng
- `it.mai_male_vits` (548MB) - High-quality Italian male, requires espeak-ng
- `es.mai_tacotron2` (362MB) - Reliable Spanish, no dependencies

**Cache Locations**:
- macOS: `~/Library/Application Support/tts`
- Linux: `~/.local/share/tts` or `~/.cache/tts`
- Windows: `%APPDATA%\tts`

### **Integration Patterns**

**Simple Integration**:
```python
from abstractvoice import VoiceManager

# One-liner: ensure models are ready and use TTS
vm = VoiceManager()
if vm.ensure_ready():
    vm.speak("Ready to go!")
```

**Robust Integration**:
```python
def setup_voice_system():
    vm = VoiceManager()

    if vm.is_model_ready():
        return vm

    print("📥 Downloading essential model...")
    if vm.ensure_ready():
        return vm

    print("❌ TTS setup failed")
    return None

vm = setup_voice_system()
if vm:
    vm.speak("System ready!")
```

**Enterprise Deployment**:
```bash
# In deployment script
abstractvoice download-models --essential
abstractvoice download-models --language fr
abstractvoice download-models --status
```

## QUICK START EXAMPLES

### **1. Basic Setup**
```python
from abstractvoice import VoiceManager

# Simplest initialization (automatic model selection)
vm = VoiceManager()

# Language-specific setup
vm = VoiceManager(language='fr')    # French with optimal defaults
vm = VoiceManager(language='it')    # Italian with speed optimization
vm = VoiceManager(language='de')    # German with premium quality

# With debug information
vm = VoiceManager(debug_mode=True)
```

### **2. Smart Model Selection**
```python
# AbstractVoice automatically selects the best model for your system:
# - Tries VITS (premium quality) if espeak-ng available
# - Falls back to Tacotron2 (universal compatibility) if needed
# - Shows model selection in debug mode

vm = VoiceManager(language='en', debug_mode=True)
# Output: ✨ Using premium quality model: tts_models/en/ljspeech/vits
#         🌍 Using English voice: tts_models/en/ljspeech/vits
```

### **3. Voice Discovery and Selection (Auto-Download in v0.5.0+)**
```python
# List all available voices
vm = VoiceManager()
vm.list_voices()

# Browse specific language
vm.list_voices('fr')

# Set specific voice - DOWNLOADS AUTOMATICALLY if needed (v0.5.0+)
vm.set_voice('fr', 'css10_vits')        # Downloads French model if needed
vm.set_voice('it', 'mai_male_vits')     # Downloads Italian model if needed
vm.set_voice('de', 'thorsten_vits')     # Downloads German model if needed
vm.set_voice('en', 'jenny')             # Downloads Jenny voice if needed
vm.set_voice('en', 'ek1')               # Downloads Edward voice if needed

# Progress indicators shown during download:
# 📥 Downloading tts_models/fr/css10/vits...
#    This may take a few minutes depending on your connection...
# ✅ Downloaded tts_models/fr/css10/vits in 45.2s
```

**New in v0.5.0:** Voice switching now downloads missing models automatically. No manual `download_model()` calls required!

## CORE OPERATIONS

### **1. Text-to-Speech**
```python
# Basic speech synthesis
vm = VoiceManager()
success = vm.speak("Hello, I am your AI assistant!")

# With speed control (preserves pitch)
vm.speak("This is normal speed", speed=1.0)
vm.speak("This is faster", speed=1.5)
vm.speak("This is slower", speed=0.7)

# With completion callback
def on_complete():
    print("Speech finished!")

vm.speak("Text with callback", callback=on_complete)

# Check if currently speaking
if vm.is_speaking():
    print("TTS is active")
```

### **2. Language Support (Auto-Download in v0.5.0+)**
```python
# Language-specific instances
vm_en = VoiceManager(language='en')    # English
vm_fr = VoiceManager(language='fr')    # French (downloads model if needed)
vm_es = VoiceManager(language='es')    # Spanish (downloads model if needed)
vm_de = VoiceManager(language='de')    # German (downloads model if needed)
vm_it = VoiceManager(language='it')    # Italian (downloads model if needed)

# Speak in different languages
vm_fr.speak("Bonjour! Je suis votre assistant IA.")
vm_es.speak("¡Hola! Soy tu asistente de IA.")
vm_de.speak("Hallo! Ich bin Ihr KI-Assistent.")
vm_it.speak("Ciao! Sono il tuo assistente IA.")

# Dynamic language switching - NOW WITH AUTO-DOWNLOAD (v0.5.0+)
vm = VoiceManager()

# Switches to French, downloads model if not cached
vm.set_language('fr')
vm.speak("Maintenant je parle français.")

# Switches back to English
vm.set_language('en')
vm.speak("Now I speak English.")

# Clear error messages if download fails:
# ❌ Cannot switch to French: Model download failed
#    Try: abstractvoice download-models --language fr
```

**New in v0.5.0:** Language switching now automatically downloads missing models with progress indicators. No more silent fallbacks to English!

### **3. Professional Audio Control**
```python
# Immediate pause/resume (20ms response time)
vm.speak("This is a long text that can be paused and resumed seamlessly.")

# Pause (stops within 20ms)
if vm.pause_speaking():
    print("✅ Paused successfully")

# Resume from exact position (no repetition)
if vm.resume_speaking():
    print("✅ Resumed from exact position")

# Check states
if vm.is_speaking():
    print("Currently playing")
if vm.is_paused():
    print("Currently paused")

# Stop completely
vm.stop_speaking()
```

### **4. Voice Recognition (Speech-to-Text)**
```python
# Basic speech recognition
def handle_speech(text):
    print(f"User said: {text}")
    vm.speak(f"I heard: {text}")

def handle_stop():
    print("User said 'stop'")
    vm.stop_speaking()

# Start listening
vm.listen(
    on_transcription=handle_speech,
    on_stop=handle_stop
)

# Check listening state
if vm.is_listening():
    print("Actively listening")

# Stop listening
vm.stop_listening()
```

## ADVANCED FEATURES

### **Unified CLI Interface (v0.3.0+)**
```bash
# Voice mode (interactive conversation with AI)
abstractvoice                      # Default: interactive voice mode
abstractvoice --model cogito:3b    # With custom Ollama model
abstractvoice --language fr        # French voice mode
abstractvoice --no-listening       # TTS-only mode

# Examples and utilities
abstractvoice cli                  # CLI REPL example
abstractvoice web                  # Web API server example
abstractvoice simple               # Simple usage demonstration
abstractvoice check-deps           # Dependency compatibility check
abstractvoice help                 # Show available commands

# Language-specific examples
abstractvoice cli --language de    # German CLI REPL
abstractvoice simple --language fr # French simple demo

# Get help
abstractvoice --help               # Complete help with all options
```

**Major Improvement in v0.3.0**: Single unified `abstractvoice` command replaces the previous dual entry point system (`abstractvoice` + `abstractvoice-cli`). All functionality is now accessible through one intuitive command.

### **Voice Management**
```python
# Get current language information
info = vm.get_language_name()
print(f"Current: {info}")

# List supported languages
supported = vm.get_supported_languages()
print(f"Supported: {supported}")

# Change settings at runtime
vm.set_speed(1.2)           # 20% faster
vm.change_vad_aggressiveness(2)  # More sensitive voice detection
```

### **Voice Modes for Different Use Cases**
```python
# Different interaction modes
vm.set_voice_mode("full")   # Continuous listening, can interrupt TTS
vm.set_voice_mode("wait")   # Listen only when TTS is not playing
vm.set_voice_mode("stop")   # Stop TTS when user speaks
vm.set_voice_mode("ptt")    # Push-to-talk mode
```

## INTEGRATION PATTERNS

### **AI Assistant Integration**
```python
from abstractvoice import VoiceManager

class VoiceAI:
    def __init__(self, language='en'):
        self.vm = VoiceManager(language=language, debug_mode=False)
        self.conversation_active = False

    def start_conversation(self):
        """Start voice conversation with AI"""
        self.conversation_active = True
        self.vm.speak("Hello! I'm your voice assistant. How can I help you?")

        # Note: listen() blocks until stop command
        # In production, run this in a separate thread
        self.vm.listen(
            on_transcription=self.process_user_input,
            on_stop=self.end_conversation
        )

    def process_user_input(self, text):
        """Process user speech and respond"""
        if not self.conversation_active:
            return

        print(f"User said: {text}")

        # Stop current speech if user interrupts
        self.vm.stop_speaking()

        # Process with your AI system (replace with actual AI)
        ai_response = self.generate_response(text)

        # Speak response
        if self.conversation_active:
            self.vm.speak(ai_response)

    def generate_response(self, text):
        """Replace with your actual AI system (OpenAI, Ollama, etc.)"""
        # Simple example responses
        if "hello" in text.lower():
            return "Hello! Nice to meet you."
        elif "language" in text.lower():
            return "I can speak multiple languages. Try saying 'switch to French'."
        elif "french" in text.lower():
            self.vm.set_language('fr')
            return "Bonjour! Je parle français maintenant."
        else:
            return f"I heard you say: {text}. How can I help you with that?"

    def end_conversation(self):
        """End conversation gracefully"""
        self.conversation_active = False
        self.vm.speak("Goodbye! Have a great day!")

    def cleanup(self):
        """Clean up resources"""
        self.conversation_active = False
        self.vm.cleanup()

# Usage example
if __name__ == "__main__":
    ai = VoiceAI(language='en')
    try:
        ai.start_conversation()
    except KeyboardInterrupt:
        print("\nConversation interrupted")
    finally:
        ai.cleanup()
```

### **Streaming AI Responses**
```python
from abstractvoice import VoiceManager
import time

def stream_ai_with_voice(user_input, vm):
    """Stream AI response with immediate TTS for each complete sentence"""

    # Example streaming AI generator (replace with OpenAI, Ollama, etc.)
    def mock_ai_stream(text):
        """Simulate streaming AI response - replace with actual AI"""
        responses = [
            f"Let me think about your question: '{text}'. ",
            "Based on my understanding, ",
            "I can provide several insights. ",
            "First, this is an interesting topic. ",
            "Second, there are multiple perspectives to consider. ",
            "I hope this helps answer your question!"
        ]

        for response in responses:
            # Simulate streaming delay
            time.sleep(0.5)
            yield response

    # Stream and speak complete sentences immediately
    full_response = ""
    current_sentence = ""

    print(f"Processing: {user_input}")

    for chunk in mock_ai_stream(user_input):
        full_response += chunk
        current_sentence += chunk

        # Check for sentence boundaries
        if any(punct in chunk for punct in ['.', '!', '?']):
            # Find the sentence boundary
            for punct in ['.', '!', '?']:
                if punct in current_sentence:
                    # Get complete sentence up to punctuation
                    idx = current_sentence.rfind(punct)
                    if idx != -1:
                        sentence = current_sentence[:idx + 1].strip()
                        if sentence:
                            vm.speak(sentence)
                            print(f"Speaking: {sentence}")
                        current_sentence = current_sentence[idx + 1:].strip()
                        break

    # Speak any remaining text
    if current_sentence.strip():
        vm.speak(current_sentence.strip())

    return full_response

# Usage example
if __name__ == "__main__":
    vm = VoiceManager(language='en')

    user_question = "What can you tell me about artificial intelligence?"
    response = stream_ai_with_voice(user_question, vm)

    print(f"\nComplete response: {response}")
    vm.cleanup()
```

### **Multilingual AI Assistant**
```python
class MultilingualAssistant:
    def __init__(self):
        self.voices = {
            'en': VoiceManager(language='en'),
            'fr': VoiceManager(language='fr'),
            'es': VoiceManager(language='es'),
            'de': VoiceManager(language='de'),
            'it': VoiceManager(language='it')
        }
        self.current_language = 'en'

    def set_language(self, language):
        if language in self.voices:
            self.current_language = language
            return True
        return False

    def speak(self, text, language=None):
        lang = language or self.current_language
        if lang in self.voices:
            self.voices[lang].speak(text)

    def greet_in_all_languages(self):
        greetings = {
            'en': "Hello! I'm your AI assistant.",
            'fr': "Bonjour! Je suis votre assistant IA.",
            'es': "¡Hola! Soy tu asistente de IA.",
            'de': "Hallo! Ich bin Ihr KI-Assistent.",
            'it': "Ciao! Sono il tuo assistente IA."
        }

        for lang, greeting in greetings.items():
            self.speak(greeting, language=lang)
            time.sleep(3)

# Usage
assistant = MultilingualAssistant()
assistant.set_language('fr')
assistant.speak("Bienvenue!")
```

## PRODUCTION DEPLOYMENT

### **Error Handling & Fallbacks**
```python
def create_robust_voice_manager():
    """Production-ready VoiceManager with fallbacks"""
    try:
        # Try premium quality first
        vm = VoiceManager(debug_mode=False)
        print("✅ Voice system ready")
        return vm
    except Exception as e:
        print(f"⚠️ Voice initialization failed: {e}")
        return None

def safe_voice_operation(vm, operation, *args, **kwargs):
    """Safely execute voice operations"""
    try:
        if operation == "speak":
            return vm.speak(*args, **kwargs)
        elif operation == "pause":
            return vm.pause_speaking()
        elif operation == "resume":
            return vm.resume_speaking()
        elif operation == "stop":
            return vm.stop_speaking()
        return False
    except Exception as e:
        print(f"Voice operation failed: {e}")
        return False

# Usage
vm = create_robust_voice_manager()
if vm:
    success = safe_voice_operation(vm, "speak", "Hello world")
    if not success:
        print("Speech failed, using text fallback")
```

### **Resource Cleanup**
```python
def cleanup_voice_resources(vm):
    """Proper cleanup for production environments"""
    try:
        vm.stop_speaking()
        vm.stop_listening()
        vm.cleanup()
        print("✅ Voice resources cleaned up")
    except Exception as e:
        print(f"⚠️ Cleanup warning: {e}")

# Context manager for automatic cleanup
from contextlib import contextmanager

@contextmanager
def voice_manager_context(**kwargs):
    """Context manager for automatic resource management"""
    vm = VoiceManager(**kwargs)
    try:
        yield vm
    finally:
        cleanup_voice_resources(vm)

# Usage
with voice_manager_context(language='fr') as vm:
    vm.speak("Bonjour! This will auto-cleanup when done.")
```

## ARCHITECTURE NOTES

### **Model Storage & Caching**
- **Cache Location**: `~/Library/Application Support/tts/` (macOS), equivalent on other platforms
- **Storage Requirements**: ~6GB for all languages (one-time download)
- **Offline Operation**: 100% offline after initial model download
- **Model Selection**: Automatic VITS → Tacotron2 fallback based on espeak-ng availability

### **Dependencies**
```python
# Core dependencies (always required)
numpy>=1.24.0
requests>=2.31.0

# Optional dependencies (install as needed)
# Voice functionality
sounddevice>=0.4.6    # Audio I/O
webrtcvad>=2.0.10     # Voice activity detection
PyAudio>=0.2.13       # Audio interface
soundfile>=0.12.1     # Audio file handling

# TTS functionality
coqui-tts>=0.27.0     # Text-to-speech engine
torch>=2.0.0          # PyTorch for TTS models
torchaudio>=2.0.0     # Audio processing for PyTorch
librosa>=0.10.0       # Audio analysis

# STT functionality
openai-whisper>=20230314  # Speech recognition
tiktoken>=0.6.0       # Tokenizer for Whisper

# Web functionality
flask>=2.0.0          # Web API server
```

### **Performance Characteristics**
- **Model Loading**: 2-5 seconds (cached after first use)
- **Speech Synthesis**: Real-time to 2x real-time depending on model
- **Pause/Resume Latency**: <20ms
- **Memory Usage**: ~500MB-2GB depending on models loaded
- **CPU Usage**: Moderate (can run on CPU, GPU optional for speed)

## TROUBLESHOOTING

### **Common Issues & Solutions (2024-2025)**

#### **1. PyTorch/TorchVision Conflicts**
**Error**: `RuntimeError: operator torchvision::nms does not exist`

**Root Cause**: Incompatible PyTorch/TorchVision versions (common in conda environments)

**Solution**:
```bash
# Step 1: Check current state
abstractvoice check-deps

# Step 2: Clean reinstall
pip uninstall torch torchvision torchaudio transformers
pip install abstractvoice[all]

# Step 3: Verify fix
python -c "from abstractvoice import VoiceManager; print('✅ Fixed!')"
```

#### **2. Import Errors After Installation**
**Error**: `ModuleNotFoundError: Could not import module 'GPT2PreTrainedModel'`

**Enhanced Error Messages**: AbstractVoice now provides specific installation commands:
```
❌ PyTorch/TorchVision version conflict detected!

To fix:
1. pip uninstall torch torchvision torchaudio transformers
2. pip install abstractvoice[all]  # Installs tested compatible versions
```

#### **3. Audio Device Issues**
**Error**: `OSError: PortAudio library not found`

**Solution**:
```bash
# macOS: brew install portaudio
# Linux: sudo apt-get install portaudio19-dev
# Windows: Usually works, try conda install pyaudio if needed

pip install abstractvoice[audio-only]  # Reinstall audio components
```

#### **4. Conda/Pip Environment Conflicts**
**Best Practice**: Use conda for environment, pip for packages
```bash
conda create -n abstractvoice python=3.10
conda activate abstractvoice
pip install abstractvoice[voice-full]  # Use pip, not conda

# Test installation
abstractvoice check-deps
```

### **Debug Mode & Dependency Checking**
```python
# Enhanced debug output
vm = VoiceManager(debug_mode=True)
# Shows model selection and compatibility info

# Comprehensive dependency check
from abstractvoice.dependency_check import check_dependencies
check_dependencies()  # Full compatibility report with fix recommendations

# Quick installation verification
abstractvoice check-deps
```

### **Legacy Issues**
1. **espeak-ng Missing**: VITS models fallback to Tacotron2 automatically
2. **Permission Errors**: Grant microphone access on macOS/Windows
3. **Model Download**: ~500MB per language, stored in `~/.cache/huggingface/`

## LICENSING & COMPLIANCE

### **Voice Model Licenses**
- **VITS Models**: Require espeak-ng (GPL v3+ with linking exception)
- **Tacotron2 Models**: No additional dependencies, standard open source
- **Commercial Use**: ✅ All models support commercial applications
- **Attribution**: Required for some datasets (see docs/voices-and-licenses.md)

### **Deployment Guidelines**
- **Embedded Systems**: Use Tacotron2 models for universal compatibility
- **Enterprise**: VITS models provide best quality when espeak-ng can be installed
- **Cloud/Serverless**: Consider model download time on cold starts

---

**Complete documentation and examples available at:**
- **GitHub Repository**: https://github.com/lpalbou/abstractvoice
- **Voice Models & Licensing**: docs/voices-and-licenses.md
- **Installation Guide**: docs/installation.md
- **Architecture Documentation**: docs/architecture.md
- **CLI Reference**: Run `abstractvoice --help` for all available commands and options
- **Voice Selection**: Use `/setvoice` command in CLI or `vm.list_voices()` in Python

**Quick verification of your installation:**
```bash
# Test import
python -c "from abstractvoice import VoiceManager; vm = VoiceManager(); print('✅ AbstractVoice installed correctly')"

# Check dependencies
abstractvoice check-deps

# Test simple functionality
abstractvoice simple
```
