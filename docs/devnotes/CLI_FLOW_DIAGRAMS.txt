╔════════════════════════════════════════════════════════════════════════════╗
║         AbstractVoice CLI Flow Diagrams & Architecture Guide               ║
╚════════════════════════════════════════════════════════════════════════════╝


═══════════════════════════════════════════════════════════════════════════════
1. ENTRY POINT ROUTING (pyproject.toml Configuration)
═══════════════════════════════════════════════════════════════════════════════

    User Command                Package Entry Point                Handler
    ───────────────            ──────────────────────             ────────

    abstractvoice          →    voice_cli.py:main           →     VoiceREPL
    ├─ --model             |    (Voice Mode)               |      (from cli_repl.py)
    ├─ --api               |                               |      Direct Ollama
    ├─ --language          |                               |      Integration
    └─ --debug             |                               └─→    REPL cmdloop()

    abstractvoice-cli      →    __main__.py:main           →      Example
    ├─ cli                 |    (Dispatcher)               |      Dispatcher
    ├─ web                 |                               |      Route to:
    ├─ simple              |                               ├─→    cli_repl.py:main
    └─ check-deps          |                               ├─→    web_api.py:main
                           |                               ├─→    simple_example()
                           |                               └─→    check_dependencies()

    python -m abstractvoice →   __main__.py (as module)    →      Example
    ├─ check-deps          |    (Dispatcher)               |      Dispatcher
    ├─ simple              |                               |      (same as above)
    └─ cli                 |                               └─→    Route to examples


═══════════════════════════════════════════════════════════════════════════════
2. VOICE MODE FLOW (abstractvoice command)
═══════════════════════════════════════════════════════════════════════════════

    START: abstractvoice [options]
           │
           ↓
    ┌──────────────────────────────┐
    │ parse_args() in voice_cli.py │  Parse CLI arguments
    │ - model (def: granite3.3:2b) │  - api URL
    │ - language                   │  - temperature
    │ - debug mode                 │  - etc.
    └──────────┬───────────────────┘
               │
               ↓
    ┌──────────────────────────────────┐
    │ Initialize VoiceREPL object      │  Create REPL instance
    │ (from cli_repl.py)               │  - Initialize voice_manager
    │                                  │  - Set model, API, language
    │ VoiceManager initialization:     │  - Load system prompt
    │  ├─ TTS Engine (lazy import)     │
    │  ├─ STT Recognizer (lazy import) │
    │  └─ VAD Engine                   │
    └──────────┬───────────────────────┘
               │
               ↓
    ┌──────────────────────────────────┐
    │ repl.cmdloop()                   │  Start interactive REPL
    │ - Displays welcome message       │
    │ - Shows available commands       │
    │ - Waits for user input           │
    └──────────┬───────────────────────┘
               │
         ┌─────┴─────┐
         │           │
         ↓           ↓
    User input   User commands
    (text)       (starting with /)
         │           │
         ↓           ↓
    ┌─────────────────────────────────┐
    │ process_query()                 │  /voice, /model, /language,
    │ Send to Ollama API:             │  /setvoice, /help, etc.
    │                                 │
    │ 1. Add message to history       │  → Handle specific commands
    │ 2. Create JSON payload:         │     (execute immediately)
    │    {                            │
    │      "model": "granite3.3:2b",  │
    │      "messages": [...],         │
    │      "temperature": 0.4,        │
    │      "max_tokens": 4096         │
    │    }                            │
    │ 3. POST to Ollama at:           │
    │    http://localhost:11434/      │
    │    api/chat                     │
    │ 4. Wait for response            │
    │ 5. Parse JSON response          │
    │ 6. Extract assistant content    │
    │ 7. Print response               │
    │ 8. Speak via voice_manager      │
    │                                 │
    └──────────┬───────────────────────┘
               │
         ┌─────┴───────┐
         │             │
         ↓             ↓
    Continue Loop    Exit (/exit)
         │             │
         └─────┬───────┘
               ↓
        CLEANUP & END


═══════════════════════════════════════════════════════════════════════════════
3. OLLAMA API INTEGRATION (Detailed Request/Response)
═══════════════════════════════════════════════════════════════════════════════

    AbstractVoice              HTTP Network           Ollama Server
    Client Code                 Boundary              (Port 11434)
    ───────────                ─────────             ─────────────

    User types: "Hello"
         │
         ↓
    payload = {
      "model": "granite3.3:2b",
      "messages": [
        {"role": "system", "content": "You are a helpful assistant..."},
        {"role": "user", "content": "Hello"}
      ],
      "stream": false,
      "temperature": 0.4,
      "max_tokens": 4096
    }
         │
         ↓ requests.post()
    ┌────────────────────────────────────────┐
    │ POST /api/chat                         │ ────────────→  Receives request
    │ Host: localhost:11434                  │
    │ Content-Type: application/json         │ ────────────→  Loads model
    │ [JSON payload above]                   │
    │                                        │ ────────────→  Generates response
    │                                        │
    ◄────────────────────────────────────────┤ ◄────────────  Sends response
    │ HTTP/1.1 200 OK                        │
    │ Content-Type: application/json         │
    │                                        │
    │ {                                      │
    │   "model": "granite3.3:2b",            │
    │   "created_at": "2024-10-19T...",      │
    │   "message": {                         │
    │     "role": "assistant",               │
    │     "content": "Hello! How can I help?"│
    │   },                                   │
    │   "done": true,                        │
    │   "prompt_eval_count": 15,             │
    │   "eval_count": 23                     │
    │ }                                      │
    └────────────────────────────────────────┘
         ↓
    Parse JSON response
         ↓
    Extract: response_data["message"]["content"]
         ↓
    Add to message history
         ↓
    Print to console
         ↓
    Call voice_manager.speak(response)


═══════════════════════════════════════════════════════════════════════════════
4. ERROR HANDLING FLOW
═══════════════════════════════════════════════════════════════════════════════

    INITIALIZATION ERRORS:
    ──────────────────────
    
    Missing TTS Dependencies
         │
         ↓ (on voice_manager.speak())
    lazy import tries to load TTSEngine
         │
         ↓ ImportError caught
    └─→  Helpful error message:
         "TTS functionality requires optional dependencies.
          Install with: pip install abstractvoice[tts]"


    OLLAMA CONNECTION ERRORS:
    ─────────────────────────
    
    requests.post(api_url, json=payload)
         │
    ┌────┴────┐
    │          │
    Connection  │ Success
    Error       │  │
    │           │  ↓
    ↓           HTTP 200
    │           │
    ├─→ ConnectionError ├─→ Parse response
        caught      │
    │              ├─→ Check format:
    └─→ Print:          - Ollama format
        "Error:          - OpenAI format
         [errno]"        - Other format


    MODEL NOT FOUND ERRORS:
    ──────────────────────
    
    requests.post() with non-existent model
         │
         ↓
    Ollama returns error (usually 404 or 400)
         │
         ↓ response.raise_for_status()
    HTTPError raised
         │
         ↓ except Exception as e:
    Caught in generic handler
         │
         ↓
    Print: "Error: 404 Client Error: Not Found for url: ..."
         │
         ↓ (if --debug)
    Print full traceback


═══════════════════════════════════════════════════════════════════════════════
5. DEPENDENCY CHECK FLOW
═══════════════════════════════════════════════════════════════════════════════

    Command: python -m abstractvoice check-deps
    or: abstractvoice-cli check-deps
             │
             ↓
    __main__.py:main()
             │
             ↓ parse arguments
    args.example == "check-deps"
             │
             ↓
    from abstractvoice.dependency_check import check_dependencies
    check_dependencies(verbose=True)
             │
             ↓
    Create DependencyChecker instance
             │
             ├─→ check_core_dependencies()
             │   ├─ numpy
             │   └─ requests
             │
             ├─→ check_pytorch_ecosystem()
             │   ├─ torch
             │   ├─ torchvision
             │   └─ torchaudio
             │
             ├─→ check_optional_dependencies()
             │   ├─ coqui-tts
             │   ├─ openai-whisper
             │   ├─ sounddevice
             │   ├─ librosa
             │   ├─ flask
             │   ├─ webrtcvad
             │   ├─ PyAudio
             │   ├─ soundfile
             │   └─ tiktoken
             │
             └─→ check_pytorch_conflicts()
                 └─ Detect known version incompatibilities
                 │
                 ↓
    For each: importlib.import_module(package)
             │
             ├─ SUCCESS: version = module.__version__
             │           compare against range
             │           report: ✅ installed, compatible
             │
             └─ FAILURE: report: ❌ missing/incompatible
                         suggest fix
                 │
                 ↓
    Print formatted report:
    ┌────────────────────────────┐
    │ 🔍 Dependency Check Report │
    │ ✅ numpy: v1.24.3         │
    │ ❌ torch: not installed    │
    │ ⚠️  flask: v2.0.1 (old)    │
    │                            │
    │ 💡 Recommendations:        │
    │    pip install [packages]  │
    └────────────────────────────┘


═══════════════════════════════════════════════════════════════════════════════
6. PARAMETER FLOW THROUGH SYSTEM
═══════════════════════════════════════════════════════════════════════════════

    abstractvoice --model mistral --temperature 0.7 --language fr
         │
         ├─ --model mistral
         │  └─→ voice_cli.py:parse_args()
         │     └─→ args.model = "mistral"
         │        └─→ VoiceREPL(model="mistral")
         │           └─→ self.model = "mistral"
         │              └─→ Used in every API request
         │
         ├─ --temperature 0.7
         │  └─→ parse_args()
         │     └─→ args.temperature = 0.7
         │        └─→ VoiceREPL(temperature=0.7)
         │           └─→ self.temperature = 0.7
         │              └─→ payload["temperature"] = 0.7
         │
         └─ --language fr
            └─→ parse_args()
               └─→ args.language = "fr"
                  └─→ VoiceREPL(language="fr")
                     └─→ voice_manager.set_language("fr")
                        └─→ Loads French TTS model
                           └─→ All speak() calls use French


═══════════════════════════════════════════════════════════════════════════════
7. MESSAGE HISTORY & CONTEXT MAINTENANCE
═══════════════════════════════════════════════════════════════════════════════

    VoiceREPL.__init__()
         │
         ↓
    self.messages = [
      {"role": "system", "content": "You are a helpful voice assistant..."}
    ]
         │
         User: "Hello"
         │
         ↓ process_query("Hello")
    self.messages.append({"role": "user", "content": "Hello"})
         │
         self.messages = [
           {"role": "system", ...},
           {"role": "user", "content": "Hello"}
         ]
         │
         ↓ POST all messages to Ollama
         │
         Ollama Response: "Hi there! How can I help?"
         │
         ↓ parse response
    self.messages.append({"role": "assistant", "content": "Hi there!..."})
         │
         self.messages = [
           {"role": "system", ...},
           {"role": "user", "content": "Hello"},
           {"role": "assistant", "content": "Hi there!..."}
         ]
         │
         User: "What's the weather?"
         │
         ↓ process_query("What's the weather?")
         │
         ↓ POST all 3 messages to Ollama
         Ollama sees conversation history
         Responds in context


═══════════════════════════════════════════════════════════════════════════════
8. VOICE MODE STATE TRANSITIONS
═══════════════════════════════════════════════════════════════════════════════

    START
      │
      ↓
    voice_mode = "off"
    voice_mode_active = False
      │
      │ User: /voice wait
      ↓
    ┌────────────────┐
    │ VOICE ENABLED  │ ←──── /voice full, wait, stop, ppt
    │ (Listening on) │
    │ voice_mode_active = True
    └────────┬───────┘
             │
             ├─→ Voice detected (VAD)
             │   │
             │   ↓
             │  Speech-to-text
             │  (Whisper)
             │   │
             │   ↓
             │  _voice_callback(text)
             │   │
             │   ↓
             │  Process message
             │   │
             │   ↓
             │  If text == "stop"
             │   │
             │   └─→ _voice_stop_callback()
             │       │
             │       ↓
             │   voice_mode = "off"
             │   voice_mode_active = False
             │       │
             │       └────→ Back to listening
             │
             └─→ User: /voice off
                 │
                 ↓
             stop_listening()
                 │
                 ↓
             voice_mode_active = False


═══════════════════════════════════════════════════════════════════════════════
9. TTS/STT INTERACTION FLOW
═══════════════════════════════════════════════════════════════════════════════

    process_query(user_text)
         │
         ↓
    Ollama generates response
         │
         ↓
    If use_tts == True:
         │
         ├─→ voice_manager.speak(response_text)
         │   │
         │   ├─→ Convert text to speech (Coqui-TTS)
         │   │
         │   ├─→ Play audio file (sounddevice)
         │   │
         │   └─→ Set is_speaking() = True
         │
         └─→ While speaking:
             │
             └─→ If user speaks (VAD detected):
                 │
                 ├─→ Interrupt TTS
                 │
                 └─→ Listen (in "full" mode)


═══════════════════════════════════════════════════════════════════════════════
10. KEY FILES & THEIR RESPONSIBILITIES
═══════════════════════════════════════════════════════════════════════════════

    pyproject.toml
    ├─ Defines CLI entry points
    ├─ abstractvoice → voice_cli.py:main
    └─ abstractvoice-cli → __main__.py:main

    __main__.py
    ├─ Example dispatcher
    ├─ Routes commands to examples
    ├─ Handles: cli, web, simple, check-deps
    └─ DOES NOT handle direct voice mode

    voice_cli.py
    ├─ Voice mode entry point
    ├─ Parses CLI arguments
    ├─ Initializes VoiceREPL
    ├─ Direct Ollama integration
    └─ DOES NOT handle examples/utilities

    cli_repl.py
    ├─ VoiceREPL class (interactive REPL)
    ├─ Ollama API interaction (requests)
    ├─ Message history management
    ├─ REPL commands implementation
    └─ Voice manager integration

    voice_manager.py
    ├─ High-level voice orchestration
    ├─ TTS engine wrapper
    ├─ STT recognizer wrapper
    ├─ Language management
    └─ Voice activity detection

    dependency_check.py
    └─ Validates all dependencies


═══════════════════════════════════════════════════════════════════════════════

Last Updated: 2024-10-19
